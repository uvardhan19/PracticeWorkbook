{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow DL Theory",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9_1yaGZH3nd"
      },
      "source": [
        "# **------------------------------------------------------------**\n",
        "# **Deep Learning With Tensorlow.Keras**\n",
        "# **------------------------------------------------------------**\n",
        "## Making Complex to Simple is the real **\"Key\"**\n",
        "## You do not need to memorize any formula or equation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWcMaV8mRxiV"
      },
      "source": [
        "### What is AI\n",
        "\n",
        "### How Humans Learn and How AI\n",
        "\n",
        "### Limitation of AI TODAY\n",
        "\n",
        "### What is Deep Learning\n",
        "\n",
        "### What is NN\n",
        "\n",
        "### Input Layer, Hidden Layer and Output Layer\n",
        "\n",
        "#### Perceptron - A simple single NN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLBwxXX1i3xg"
      },
      "source": [
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "### How NN works, Its math and eqations.\n",
        "* x-input\n",
        "* w-weight\n",
        "* b-bias\n",
        "* z-activation function\n",
        "* Y=Submition [ Xi*Yi ]+Bias\n",
        "* Y=X1W1 + X2W2 X3Y3 + ...+ XnYn + bias\n",
        "* Z=Activation Fun(Y)\n",
        "\n",
        "\n",
        "# Neurons Functionality\n",
        "##### Two things happens at Neurons \n",
        "* 1.Weight * Input submision\n",
        "* 2.Activation Funtion get activated if value greater than p=.05\n",
        "\n",
        "Input O ->- O->- O  output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3pepCRGRGlh"
      },
      "source": [
        "# Lib For Deep Learning\n",
        "\n",
        "**TensorFlow       |     Keras     |   Pytorch**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByNPU2l4SpAI"
      },
      "source": [
        "### ANN - Artificial NN      - Any Ml can be done in ANN\n",
        "### CNN - Convolutional NN - Img n Video processing | Computer Vission is also in this domain.\n",
        "\n",
        "### RNN - Recurrent neural network (RNN), Word2Vec, Word Embeddings,LSTM,GRU,BIDIRECTIONAL LSTM\n",
        "\n",
        "#### **Other out of box**\n",
        "RCNN, Masked RCNN, SSD, Yolo- Object Detection\n",
        "Transfer Learning Tech Eg ALEXNET,VGG16\n",
        "DNLP- Attention Model and other famus models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Aj0Bs6xSoOa"
      },
      "source": [
        "# ------------------------------------------------------------------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lXkqXOdyk1S"
      },
      "source": [
        "# Activation Function\n",
        "\n",
        "### why activation fun?\n",
        "* They add a non linear properties to our network. \n",
        "* linear are straight line or plane, but non linear are curve in nature which is more real and practical approch. \n",
        "* NN are called Universal Function Approximator- Means they can compute any fuction e.i. any process can be converted to function can be converted to NN\n",
        "\n",
        "Most used and famus activation Functions are \n",
        "* **TanH** range -1 to +1, f(x)= (2/ 1+e ^-2x ) -1, Derivative output is always less then 1.\n",
        "* **Relu** -Rectified Linear Unit, f(x)=Max(0,x),0 to infinity,never negative\n",
        "* **Sigmoid** f(x)=1/1-e^-x-- one of the first to be used but caused problems like Gradient Vanishing and Gradient Exploding. Range 0 to 1. Threshold 0.5 for p\n",
        "##### -----------------------------------------------------------------\n",
        "##### **Points**\n",
        "* Activation Function in one  layer for each neuron is same, 1 layer have 1 activation function only\n",
        "* Relu is most famus and almost used every where.\n",
        "* At the output layer classification(Multiclass) use softmax Funxtion (to give probablity of each class) and in regression we use some linear activation function.\n",
        "\n",
        "# Linear Activation Function:\n",
        "*A linear activation function takes the form: A = cx. It takes the inputs, multiplied by the weights for each neuron, and creates an output signal proportional to the input. In one sense, a linear function is better than a step function because it allows multiple outputs, not just yes and no.\n",
        "\n",
        "* However, a linear activation function has two major problems:\n",
        "\n",
        "  * 1. Not possible to use backpropagation  (gradient descent) to train the model—the derivative of the function is a constant, and has no relation to the input, X. So it’s not possible to go back and understand which weights in the input neurons can provide a better prediction.\n",
        "\n",
        "\n",
        "  * 2. All layers of the neural network collapse into one—with linear activation functions, no matter how many layers in the neural network, the last layer will be a linear function of the first layer (because a linear combination of linear functions is still a linear function). So a linear activation function turns the neural network into just one layer.\n",
        "\n",
        "A neural network with a linear activation function is simply a linear regression model. It has limited power and ability to handle complexity varying parameters of input data.\n",
        "#-------------------------------------------------------------\n",
        "#### Now a big value gradient can make the neuron die-- it never gets activated\n",
        "* To over come this a Leaky relu was introduced.\n",
        "* A little negative side was introduced instead of zero.\n",
        "* Leaky Relu ranged from -Infinity to +Infinity\n",
        "---\n",
        "---\n",
        "* in output layer classifier use softmax n neurons are equals to number of classes, in regression we use linear activation fun.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgaXxWmfc126"
      },
      "source": [
        "# Loss Function\n",
        "* ACtual Value - Predected Value | (Y-Y_hat)^2\n",
        "\n",
        "# Cost Function \n",
        "* avg of all loss fun value Submision 1 to n (Y-Y_hat)^2\n",
        "\n",
        "The terms cost and loss functions almost refer to the same meaning. But, loss function mainly applies for a single training set as compared to the cost function which deals with a penalty for a number of training sets or the complete batch. It is also sometimes called an error function.\n",
        "\n",
        "In short, we can say that the loss function is a part of the cost function. The cost function is calculated as an average of loss functions. The loss function is a value which is calculated at every instance. \n",
        "\n",
        "So, for a single training cycle loss is calculated numerous times, but the cost function is only calculated once.\n",
        "\n",
        "# Front propagation as above y= wx+b=output\n",
        "\n",
        "# Back propagation    W_new = W_old-L*derivative(Loss Function) / With respect to Derivative of W_old\n",
        "  * W_new = W_old-L*d(L) / d(W_old) where l-learning rate is a very small number eg 0.001\n",
        "  * Derivative help us finding the slop,if its -ve or +ve,if line is down its -ve and if line is up its +ve.(Right hand side of that tangent line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8vz_e8HiVNU"
      },
      "source": [
        "# Multi Layer NN\n",
        "* Multiple Hidden layers\n",
        "* Matrix is formed eg: 3*4 means 3 neurons connected to 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaQV0wzjxji2"
      },
      "source": [
        "# Chain Rule (Back propagation)\n",
        " * dL/dx = dL/du * du/dx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4-0OSKQKxhw"
      },
      "source": [
        "# **Problems in past related to Gradient**\n",
        "\n",
        "## **Vanishing Gradient Problem**\n",
        "* When you new updated weight became so small that it is niglagible and make no update in the new weight. \n",
        "* Basically New Wg ~~ Old Wg\n",
        "* We initially faced this problem due to sigmoid\n",
        "* W_new = W_old-l(der(Loss)/der(W_old), l-learning rate\n",
        "* Derivative of sigmoid is 0-.25, sigmoid range is 0-1.\n",
        "* As my layers increase the value keep getting smaller and will be very close to 0 at some point.\n",
        "* Same problem with TanH,derivative range 0-1,\n",
        "\n",
        " ## **Exploding Gradient Problem**\n",
        " * We know sigmoid range 0-1, derivative range 0-.25\n",
        " * Exp. Gra. happens due to weight initializing to a very high value efg 500,900 etc\n",
        " * In chain rule equation all value will be higher and it will make it even higher value.\n",
        " * deeper the NN more the wg * (0-0.25)\n",
        " * So it will make new weight to away from old value , gradient decent will keep jumping here and there never coming to global minima.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iKzYupZsxVO"
      },
      "source": [
        "# **Under Fitting and Over Fitting**\n",
        "* Under Fitting may happen when we have just one hidden layer\n",
        "* For a multi layered NN under Fitting is not Possible, As we have so many neurons to train with, But now a ANN may be pron to over fitting.\n",
        "* In over fitting we face High Varience problem,e.i. test results are poor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM4I_AEPsiZS"
      },
      "source": [
        "#### To over come over fitting we use **Regularization** and **Drop Out**\n",
        "# **Regularization- L1 and L2 (Lasso  and Rigid)**\n",
        "##### L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
        "* L1 and L2 (Lasso- labda*|m|  and Rigid- labda*m^2)\n",
        "\n",
        "# **Drop Out** \n",
        "* Dropout works in the same patter as Random Forest regularization technique. e.i. it will take subset of each Data for each tree.\n",
        "#### *Almost same as random forest regularization*\n",
        "* Drop Out method range from 0-1\n",
        "* In 1st propagation as per the p=0.5 value it will randomly activate and deactivate few neurons.Including Input Neurons.\n",
        "* in back propagation we update the activated neurons and again same process.\n",
        "* Every time features will be selected randomly\n",
        "\n",
        "##### All this is for training part, Now we will work on test data\n",
        "* In testing time we have all the neurons connected\n",
        "* Now weight * Probablity and so on.\n",
        "* To decide the P value we use hyper parameter.\n",
        "* If model is overfitting usually the best practice is to keep p value higher then 0.5. p>0.5 in overfitting case. To find exact value use Hyper ParaMeter like CV ,GridCV or RandCV etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9vbgmIz5NOr"
      },
      "source": [
        "# Important thing to remember\n",
        "* The NN start with completly random weight\n",
        "* Wg are different\n",
        "* A bies is also added with wg in each layer\n",
        "* The activation function for each neuron in a layer will same.\n",
        "* different layer can have different activation function ,but in same layer all Neuron will have same activation function.\n",
        "\n",
        "# **Weight Initiallization Technique Imp Points**\n",
        "* Small weight- Exploding Gradient Prob\n",
        "* Wg should not be too small - vanishing Gradien prob\n",
        "*  Do not use same weights make them unique.Other wise the learning of all neuron will be same which is useless.So each weight should be very differnt from each other to get a good output.\n",
        "\n",
        "# Weight Initiallization Technique\n",
        "* Uniform Distribution wg=Uniform[-1/SqRt(input),1/SqRt(input)] -- Lower,Uper limit - (good with sigmoid activation fun)\n",
        "\n",
        "* Zero Distribution\n",
        "* Xavir / Gorat - (good with sigmoid activation fun)\n",
        "  * Xavir Normal Distribution\n",
        "    * Normal distribution is found with mean 0 and some standard deviation (0,q)\n",
        "    q=Standard Deviation is drived by sqrt(2/(input+output)\n",
        "  * Xaviour Uniform Distribution- (good with sigmoid activation fun)\n",
        "      * Uniform Distribution is found\n",
        "      * low range , high range respectively \n",
        "      U[-sqrt(6)/sqrt(ip+op), sqrt(6)/sqrt(ip+op)]\n",
        "      wg is initiallized by this formula\n",
        "\n",
        "* He_init technique:pronunced as \"hey\"- - (good with Relu activation fun)\n",
        "    * He Uniform   U[-sqrt(6)/sqrt(ip), sqrt(6)/sqrt(ip)]\n",
        "\n",
        "    * He Normal w=ND( 0,q ), q=SD=sqrt(2/input)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBvCdZ_zpn6Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH2qX9pKpn8R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbif64lsOBgk"
      },
      "source": [
        "# **Relu Rectified Linear Units (ReLU)**\n",
        "* Formula max(0,y)  y is the output\n",
        "* A angle of 45 degree with Tan is taken == 1 for derivative @\n",
        "* range 0 to infinity\n",
        "* derivative 0 or 1, derivative of 0 cant be found\n",
        "* Now 0 will make the whole chain rule eq as 0 , which will create your dead neuron or dead activation function.\n",
        "\n",
        "## Above problem is solved by Leaky Relu\n",
        "* A little angle is introduce at negative side to avoid having a zero 0\n",
        "* If we observe a 50% or close dead neuron we should use leaky relu instead of relu\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ao1_z7f-ov9"
      },
      "source": [
        "# **Main Type of GRADIENT DESCENT**\n",
        "# BATCH GRADIENT DESCENT\n",
        "Batch gradient descent, also called vanilla gradient descent, calculates the error for each example within the training dataset, but only after all training examples have been evaluated does the model get updated. This whole process is like a cycle and it's called a training epoch.\n",
        "\n",
        "* Can be stuck in Local Minima very easily.\n",
        "* It also requires the entire training dataset be in memory and available to the algorithm.\n",
        "\n",
        "# STOCHASTIC GRADIENT DESCENT\n",
        "Stochastic gradient descent (SGD) does this for each training example within the dataset, meaning it updates the parameters for each training example **one by one**. Depending on the problem, this can make SGD faster than batch gradient descent. One advantage is the **frequent updates** allow us to have a pretty detailed rate of improvement.\n",
        "\n",
        "The frequent updates, however, are more computationally expensive than the batch gradient descent approach. Additionally, the frequency of those updates can result in noisy gradients, which may cause the error rate to jump around instead of slowly and smooth decreasing.Basically a Zig-Zag is observed.\n",
        "\n",
        "**SGD with momentum** is basically used to reduce this zig zac by using moving Average.\n",
        "\n",
        "# MINI-BATCH GRADIENT DESCENT- **Most Famus**\n",
        "Mini-batch gradient descent is the go-to method since it’s a combination of the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent.\n",
        "\n",
        "Common mini-batch sizes range between 50 and 256, but like any other machine learning technique, there is no clear rule because it varies for different applications. This is the go-to algorithm when training a neural network and it is the most common type of gradient descent within deep learning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJsjma_UNvTI"
      },
      "source": [
        "# Gradient Decent Types:   Further Details.\n",
        "---\n",
        "# Gradient Decent\n",
        "# Stochastic Gradient Decent SGD\n",
        "  * One Data Point --> Just like Linear Regression\n",
        "# Mini Batch Gradient Decent\n",
        "  * k data points\n",
        "  * Very actively used today in market \n",
        "  * Mini Batch fix the number of rows it use to find local n global minima \n",
        "  * So , instead of using 100k rows we can use 100 to save process and time.\n",
        "\n",
        "  NOTE: Derivative-d,Loss-L, \n",
        "  dL/w_old-MBGD ~~ dl/W_OLD- GD\n",
        "  * This relation mimic Cetral Limit Theory Population Mean/SD~~sample mean/sd\n",
        "\n",
        "# Convex and Non Convex Function\n",
        "* Convex usually comes in Regression. U- **parabola curve**\n",
        "* Non Convex in Deep Learning Method. UnunUn\n",
        "DL has multiple Neurons and all need to converge so we have multiple points.\n",
        "\n",
        "#### A very imp factor play role here e.i. Global and Local Minima and Maxima , Saddle Point eg ----,.\n",
        "  \n",
        " * Loacl minima is the lowest converge point in that locality\n",
        " * The lowerst one is called local minima\n",
        " * same for maxima\n",
        " * Saddle Point - flat low area with further lower points, \n",
        " a point at which a function of two variables has partial derivatives equal to zero but at which the function has neither a maximum nor a minimum value.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGbG3TO0GdPw"
      },
      "source": [
        "# ***Optimizers In NN***\n",
        "---\n",
        "\n",
        "\n",
        "> Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. Optimizers help to get results faster too.\n",
        "\n",
        "Sparse data set- when data has 0 more then it has ones or majority is zero\n",
        "Dense data set-  when data has more 1 then zero ,majority is 1 \n",
        "\n",
        "for both above case learning rate cant be same if we want optimal result.\n",
        "\n",
        "### Point Of discussion:\n",
        "1. AdaGrad -- Adaptive Gradient- Using Different Learning rates for each layer|neuron based on different Iterations.\n",
        "So learning rate change w.r.t weight and iteration.\n",
        "\n",
        "  learning rate -lr,t-at a particular iter\n",
        "  lr_t= lr/sqrt(alpha_t + Epsilon-- some small +ve number\n",
        "  * Epsilon is to prevet from getting zero if alpha becames zero\n",
        "  * alpha_t= sumE i=1-n [der(Loss)/der(wg)]^2, LR value will be bigger--- and keeps getting smaller as alpha value keeps getting smaller.\n",
        "  * **Disadvantage** is in only one part that some time alpha can became a very big number due to sqare and make learning rate a very small number. Even though it is rare,we need to handel it.\n",
        "  We do that by using Adam Optimizer and RMSProp optimizer Both work in same way but build by different teams.\n",
        "\n",
        "2. RMSProp (RMSprop stands for Root Mean Square Propagation) **and** 3. Adam Optimizer (Adam is not an acronym and is not written as “ADAM”.The name Adam is derived from adaptive moment estimation.)\n",
        "\n",
        "Weight Average=wg_avg_at time t= gama*wg_avg_t-1  +(1-gama) [der(Loss)/der(wg_t)]^2\n",
        "where gama = 0.90 - 0.95, its 0.95 in most research cases\n",
        "\n",
        "* QUE: Which is the best Optimization Algorithm for Deep Learning?\n",
        "* APROX:Adam Optimizer has the best performance in max DL algo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knZ-blx3uN73"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If5BcRx3tD5g"
      },
      "source": [
        "# **Clustering in NN**\n",
        "---\n",
        "Neural networks are widely used in unsupervised learning in order to learn better representations of the input data. For example, given a set of text documents, NN can learn a mapping from document to real-valued vector in such a way that resulting vectors are similar for documents with similar content, i.e. distance preserving. This can be achieved using, for example, \n",
        "\n",
        "**auto-encoders** - a model that is trained to reconstruct the original vector from a smaller representation (hidden layer activations) with reconstruction error (distance from the ID function) as cost function. This process doesn't give you clusters, but it creates meaningful representations that can be used for clustering. You could, for instance, run a clustering algorithm on the hidden layer's activations.\n",
        "\n",
        "Clustering: There are a number of different NN architectures specifically designed for clustering. The most widely known is probably **self organizing maps.**\n",
        " A **SOM** is a NN that has a set of neurons connected to form a topological grid (usually rectangular). When some pattern is presented to an SOM, the neuron with closest weight vector is considered a winner and its weights are adapted to the pattern, as well as the weights of its neighbourhood. In this way an SOM naturally finds data clusters. A somewhat related algorithm is growing neural gas (it is not limited to predefined number of neurons).\n",
        "\n",
        "Another approach is Adaptive Resonance Theory where we have two layers: \"comparison field\" and \"recognition field\". Recognition field also determines the best match (neuron) to the vector transferred from the comparison field and also have lateral inhibitory connections. Implementation details and exact equations can readily found by googling the names of these models, so I won't put them here.\n",
        "\n",
        "# @ Simplify cluster explaination "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD92oX3CySoW"
      },
      "source": [
        "---\n",
        "\n",
        "# **Practical Implimentation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLZjPHY2kA9X",
        "outputId": "c1d8ee1c-e283-4816-b243-cc2477c150d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! python --version\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbzCL61ik_W3",
        "outputId": "483d3d24-3926-4b54-fac4-681ba3192208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "! pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf9PpuOUkvtC"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf01lXXGkLXh",
        "outputId": "dbbab26d-6024-428e-e289-aadc1dbf2206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvm3tl8dkqHa",
        "outputId": "6c07ec52-16bb-48a7-cce5-2cc59198e8b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r1NBgvumVBP"
      },
      "source": [
        "## **Deep Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOtkqcgLmgQ_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wfp7Qi6rI0E"
      },
      "source": [
        "\n",
        "\n",
        "*   Intro Of DL\n",
        "*   Intro Of Tensorflow\n",
        "*   Intro Of NN\n",
        "*   Loss and Activation Function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoOQNCTNzqmz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPN3P8-JzrA-"
      },
      "source": [
        "\n",
        "## Batch size (machine learning)\n",
        "\n",
        "\n",
        "Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. The batch size can be one of three options:\n",
        "\n",
        "batch mode: where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent\n",
        "mini-batch mode: where the batch size is greater than one but less than the total dataset size. Usually, a number that can be divided into the total dataset size.\n",
        "stochastic mode: where the batch size is equal to one. Therefore the gradient and the neural network parameters are updated after each sample.\n",
        "\n",
        "Stochastic (from from Greek στόχος (stókhos), meaning 'aim, guess'.) is any randomly determined process. In mathematics the terms stochastic process and random process are interchangeable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlolmLDvmr1Y"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Logical Topic List\n",
        "\n",
        "1.   Neural Networks \n",
        "2.   Activation Function\n",
        "3.   Front Prapogation\n",
        "4.   Input and Output in Deep NN\n",
        "5.   Back Prapogation\n",
        "6.   Chain Rule\n",
        "7.   Vanishing Gradient\n",
        "8.   Exploding Gradient\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_P1gjN2nJ6L",
        "outputId": "506bd005-35a2-436a-b2c3-64ead4d551b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knXPIuOGI7dI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tu5KNmbKiQK"
      },
      "source": [
        "## **About MNIST Data Set**\n",
        "\n",
        "The MNIST dataset contains 70,000 images of handwritten digits (zero to nine) that have been size-normalized and centered in a square grid of pixels. Each image is a 28 × 28 × 1 array of floating-point numbers representing grayscale intensities ranging from 0 (black) to 1 (white). The target data consists of one-hot binary vectors of size 10, corresponding to the digit classification categories zero through nine.\n",
        "eg 0000000001,0000000000\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9A0AsXoFjKL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWl2SJwqJG0e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGal3LssG_o8"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense( 128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKvNe_XjHCsX"
      },
      "source": [
        "m=mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdyoq00IReih"
      },
      "source": [
        "m.count()"
      ]
    }
  ]
}